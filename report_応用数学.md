<h1> 第１章：線形代数 </h1>  
  <h2>行列</h2>
    <p>行列とはスカラーを表に表したもので、ベクトルの変換に使うことができる。 </br></p>
  <h2>固有値・固有ベクトル</h2>
    <p>
    また、正方行列に固有値分解、長方行列に特異値分解を行うことで、</br>
    固有値、固有ベクトルを見つけることができ、</br> 
    それらは計算の高速化や、画像データの分解に用いることができる。</br>
    </p>
</br>

<h1> 第２章：確率・統計</h1>
  <h2>集合</h2>
    <p>
      統計的な事実とは様々な細かい要素からなる。</br>
      集合とは、素朴に物の集まり、もの一つ一つを要素という。</br>  
     「事象」は「集合」として取り扱うことができる。</br>
    </p>
  </br>
  <h2>確率</h2>
    <p>
      ある事象がどれくらい起こったり起こらなかったりするかを考えるのが確率。</br>
     「頻度確率」と「ベイズ確率」を表したものに分けられる。
    </p>
    <dl>
      <dt>頻度確率</dt>
      <dd>事象が発生する頻度。測定すれば決まる。客観的に調べることができる。</dd>
      <dt>ベイズ確率</dt>
      <dd>信念の度合いとして考える。条件をつかって確信を深めていく。</dd>
    </dl>
  </br>
  <h2>統計</h2>
    <p>
    統計は、記述統計と推測統計に分けられる。</br>
    記述統計…母集団の性質を要約し記述する。</br>
    推測統計…母集団の一部を抽出し、その標本からもとの母集団の性質を推測する。</br>
    </br>
    統計学で扱う数字のことを<strong>確率変数</strong>といい、</br>
    事象と結びつけられた数値、あるいは事象そのものを指すとも考えられる。</br>
    また、確率変数がでる確率の分布を<strong>確率分布</strong>、</br>
    確率分布の平均値を<strong>期待値</strong>、</br>
    データの散らばり具合を表すものを<strong>分散</strong>、</br>
    分散をもとのデータと単位をそろえるために平方根をとったものを<strong>標準偏差</strong>、</br>
    ２つのデータ系列の傾向の違いを表すものを<strong>共分散</strong>という。</br>
    </br>
    これらを用いて、母集団を特徴づける母数の推定を行う。</br>
    推定は１つの値に推定する「<strong>点推定</strong>」と、範囲を推定する「<strong>区間推定</strong>」がある。</br>
    推定を行った母数は、「標本平均」や、「標本分散」、「不変分散」などがある。</br>
    <dl>
      <dt>標本平均</dt>
      <dd>
        推定によって母集団から取り出した標本の平均値を推測した値を標本平均といい、</br>
        <strong>一致性</strong>と<strong>普遍性</strong>を満たす。</br>
        ※一致性...サンプル数が大きくなれば母集団の値に近づく。</br>
        ※不偏性...サンプル数がいくらであっても期待値は母集団と同じ。</br>
       </dd>
       <dt>標本分散・不変分散</dt>
       <dd>
        標本平均をつかって分散を求めると、一致性は満たすが、不偏性は満たさない。</br>
        そこで、不変分散を求めることで、もとの母分散に近づく。
        </dd>
      </dl>
    
</br>
</br>

<h1>第３章：情報理論</h1>
  <dl>
    <dt>自己情報量</dt>
    <dd>
      情報科学では「情報をどうやって数量化するのか」について考えるのも重要。</br>
      情報の増加を比率でとらえると、その情報の増え方を足し合わせれば全体でどれだけ情報をもっているかがわかる。</br>
      それを「自己情報量」と呼び、確率をもって表される。</br>
      この自己情報量によって「情報がどれだけ珍しいか」を数値化できる。</br>
     </dd>
    <dt>シャノンエントロピ</dt>
     <dd>
      また、自己情報量の期待値を「シャノンエントロピ」といい、</br>
      「情報源の不確かさ」を数値化することができる。これは誤差関数の中身などに使い道がある。</br>
     </dd>
    <dt>カルバック・ライブラー・ダイジェンス</dt>
    <dd>
      また同じ事象・確率変数における異なる確率分布P,Qの差異を図る尺度を表すもの。</br>
    </dd>
    <dt>交差エントロピー</dt>  
    <dd>
      確率分布Qについての自己情報量を確率分布Pで平均をとるもの。</br>
    </dd>
  </dl>
